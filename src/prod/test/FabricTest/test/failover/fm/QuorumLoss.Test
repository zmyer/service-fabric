#
# [owner] MMohsin, AnuragG
#

votes 10 20 30
cleantest

fmservice 5 3
cmservice 5 3
namingservice 1 5 3

set DummyPLBEnabled true

+10
+20
+30
+40
+50
verify

################################################################################
# Scenario 1: A majority of the replicas go down before node has achieved
#             MinReplicaSetSize
################################################################################

addbehavior b1 * * AddPrimaryReply

createservice fabric:/scenario1 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=3 replicarestartwaitduration=3600

!waitforstate RA.Replica.State.fabric:/scenario1.50.50 RD

removeruntime 50 y
!waitforstate FM.Replica.IsUp.fabric:/scenario1.50 false
!waitforstate FM.FT.QuorumLost.fabric:/scenario1 true

removebehavior b1

addruntime 50 y
verify

!waitforstate FM.Replica.Role.fabric:/scenario1.30 Secondary
!waitforstate FM.Replica.Role.fabric:/scenario1.40 Secondary
!waitforstate FM.Replica.Role.fabric:/scenario1.50 Primary

deleteservice fabric:/scenario1

verify

################################################################################
# Scenario 2: QuourmLossWaitDuration is used to restore quourm
################################################################################

createservice fabric:/scenario2 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=3 replicarestartwaitduration=3600
verify

-30
-40

!waitforstate FM.FT.QuorumLost.fabric:/scenario2 true

!pause,5
!waitforstate FM.FT.QuorumLost.fabric:/scenario2 true

updateservice fabric:/scenario2 Stateful QuorumLossWaitDuration=1

!waitforstate FM.FT.QuorumLost.fabric:/scenario2 false

+30
+40
verify

deleteservice fabric:/scenario2

verify

################################################################################
# Scenario 3: Quorum should not be restored if QuorumLossWaitDurtion is very
#             high (but not infinite)
################################################################################

createservice fabric:/scenario31 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=3 replicarestartwaitduration=3600 quorumlosswaitduration=86400
createservice fabric:/scenario32 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=3 replicarestartwaitduration=3600
verify

updateservice fabric:/scenario32 Stateful QuorumLossWaitDuration=86400
!waitforstate FM.Service.QuorumLossWaitDuration.fabric:/scenario32 86400

removeruntime 30 y
removeruntime 40 y

!waitforstate FM.FT.QuorumLost.fabric:/scenario31 true
!waitforstate FM.FT.QuorumLost.fabric:/scenario32 true

!pause,5
!waitforstate FM.FT.QuorumLost.fabric:/scenario31 true
!waitforstate FM.FT.QuorumLost.fabric:/scenario32 true

addruntime 30 y
addruntime 40 y
verify

deleteservice fabric:/scenario31
deleteservice fabric:/scenario32
verify

################################################################################
# Scenario 4: If a partition is in quorum loss, an incoming replica with an
#             higher epoch should be ignored
################################################################################

updateservice fabric:/System/FailoverManagerService Stateful TargetReplicaSetSize=1 MinReplicaSetSize=1
!waitforstate FMM.Service.TargetReplicaSetSize.FMService 1
!waitforstate FMM.Service.MinReplicaSetSize.FMService 1
verify

createservice fabric:/scenario4 TestPersistedStoreServiceType y 1 5 persist minreplicasetsize=5 replicarestartwaitduration=3600
verify

addbehavior b1 * * GetLSN
addbehavior b2 * * Deactivate

removeruntime 10 y
removeruntime 20 y
removeruntime 30 y
removeruntime 40 y
removeruntime 50 y

!waitforstate FM.Replica.IsUp.fabric:/scenario4.10 false
!waitforstate FM.Replica.IsUp.fabric:/scenario4.20 false
!waitforstate FM.Replica.IsUp.fabric:/scenario4.30 false
!waitforstate FM.Replica.IsUp.fabric:/scenario4.40 false
!waitforstate FM.Replica.IsUp.fabric:/scenario4.50 false

addruntime 50 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.50 true

addruntime 10 y
addruntime 20 y
addruntime 30 y
addruntime 40 y

!waitforstate FM.Replica.IsUp.fabric:/scenario4.10 true
!waitforstate FM.Replica.IsUp.fabric:/scenario4.20 true
!waitforstate FM.Replica.IsUp.fabric:/scenario4.30 true
!waitforstate FM.Replica.IsUp.fabric:/scenario4.40 true

removeruntime 50 y
!waitforstate FM.Replica.Role.fabric:/scenario4.40 Primary

addruntime 50 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.50 true

removeruntime 40 y
!waitforstate FM.Replica.Role.fabric:/scenario4.50 Primary

addruntime 40 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.40 true

removeruntime 50 y
!waitforstate FM.Replica.Role.fabric:/scenario4.40 Primary

addruntime 50 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.50 true

removeruntime 40 y
!waitforstate FM.Replica.Role.fabric:/scenario4.50 Primary

addruntime 40 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.40 true

removeruntime 50 y
!waitforstate FM.Replica.Role.fabric:/scenario4.40 Primary

addruntime 50 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.50 true

removeruntime 40 y
!waitforstate FM.Replica.Role.fabric:/scenario4.50 Primary

addruntime 40 y
!waitforstate FM.Replica.IsUp.fabric:/scenario4.40 true

addbehavior b3 * * DoReconfiguration

-40
-50

!waitforstate FM.FT.QuorumLost.FMService true

removebehavior b3
removebehavior b2
removebehavior b1

recoverpartition 00000000-0000-0000-0000-000000000001
verify

removeruntime 30 y
!waitforstate FM.FT.QuorumLost.fabric:/scenario4 true

+50
!waitforstate FM.Node.IsUp.50 true

!pause 5

# Replica on node 50 with higher epoch should be ignored
!waitforstate FM.Replica.IsUp.fabric:/scenario4.50 false

# Bring the partition out of quourm loss
addruntime 30 y

# Now the higher epoch replica on node 50 should have been dropped
!waitforstate FM.Replica.State.fabric:/scenario4.50 Dropped

+40

updateservice fabric:/System/FailoverManagerService Stateful TargetReplicaSetSize=1 MinReplicaSetSize=1

deleteservice fabric:/scenario4

verify

-*

!q
